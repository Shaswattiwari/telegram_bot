# -*- coding: utf-8 -*-
"""telebot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17KCM-J00qhH5CSZ8dPI0C-iEzoOSiJ8p
"""

!pip3 install -q -U accelerate==0.27.1
!pip3 install -q -U transformers==4.38.0
!pip install gemma

import IPython
IPython.Application.instance().kernel.do_shutdown(True)

import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import GemmaTokenizer

from transformers import AutoTokenizer, pipeline
import torch

model = "google/gemma-2b-it"

tokenizer = AutoTokenizer.from_pretrained(model,token='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')

pipeline = pipeline(
    "text-generation",  # Specify the task as text generation
    model=model,  # Specify the pre-trained model
    model_kwargs={"torch_dtype": torch.bfloat16},  # Additional model arguments
    device="cuda",  # Specify the device (CUDA for GPU, "cpu" for CPU)
    token='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'  # Hugging Face API token
)

messages = [
    {"role": "user", "content": "Hello"},
]
prompt = pipeline.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
outputs = pipeline(
    prompt,
    max_new_tokens=256,
    do_sample=True,
    temperature=0.7,
    top_k=50,
    top_p=0.95
)
print(outputs[0]["generated_text"][len(prompt):])

import locale
print(locale.getpreferredencoding())

def getpreferredencoding(do_setlocale = True):
 return "UTF-8"
locale.getpreferredencoding = getpreferredencoding

!pip install telebot

import telebot

bot = telebot.TeleBot("6558909327:AAFNiMFiBdCTtlJRAbRn34TmjvjhF_unhrk")

def generate_response(text):
    messages = [
        {"role": "user", "content": f"{text}"},
    ]
    prompt = pipeline.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    outputs = pipeline(
        prompt,
        max_new_tokens=256,
        do_sample=True,
        temperature=0.7,
        top_k=50,
        top_p=0.95
    )
    response = outputs[0]["generated_text"][len(prompt):]
    return response

# Handler for the /start command
@bot.message_handler(commands=['start'])
def send_welcome(message):
    bot.reply_to(message, "Welcome to YourBot! How can I assist you today?")

# Handler for all text messages
@bot.message_handler(func=lambda message: True)
def handle_message(message):

    # Generate response using the model
    response = generate_response(message.text)

    # Reply with the model's response
    bot.reply_to(message, response)

# Start the bot
bot.polling()